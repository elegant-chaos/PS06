---
title: "STAT/MATH 495: Problem Set 06"
author: "Jenn Halbleib"
date: "2017-10-17"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
    df_print: kable
---

```{r setup, include=FALSE}
# Load packages
library(tidyverse)
library(broom)
library(knitr)

opts_chunk$set(echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE, warning = FALSE)
```





# Collaboration

Please indicate who you collaborated with on this assignment: 





# Setup

Define truth, which again we know for the purposes of this assignment, but in
practice we won't:

* the true function f(x) i.e. the signal
* the true epsilon i.e. the noise, which in this case is Normal$(0, sd=\sigma)$.
Hence the standard deviation $\sigma$ determines the amount of noise.

```{r}
f <- function(x) {
  x^2
}
sigma <- 0.3
```

This is the target point we'll be trying to predict: $(0.95, f(0.95)) = (0.95, 0.95^2) = (0.95, 0.9025)$, Thus, the test set is just `x=0.95`

```{r}
x0 <- 0.95
test_set <- data_frame(x=x0)
```

This function generates a random sample of size $n$; think of this as a "get new
data" function. Random in terms of both:

* (New) the predictor x (uniform on [0,1])
* the amount of noise $\epsilon$

```{r}
generate_sample <- function(f, n, sigma) {
  sample <- data_frame(
    x = runif(n = n, min = 0, max = 1),
    f_x = f(x), #Note: f(x) = x^2 is defined above
    epsilon = rnorm(n = n, mean = 0, sd = sigma),
    y = f_x + epsilon
  )
  # Recall: We don't observe f(x) and epsilon, just (x, y)
  sample <- sample %>% 
    select(x, y)
  
  return(sample)
}
```

Define

* The number $n$ of observations $(x_i, y_i)$ in each sample. In the handout,
$n=100$ to keep plots uncrowded. Here we boost to $n=500$
* Number of samples of size $n$ to consider

```{r}
n <- 500
n_sample <- 10000
```


# Computation

```{r}
set.seed(100)

#Function estimates for test_set x values using a set of smooth.spline models with 
#df = 2 and df = 99 after generating new data and fitting models to the new data

get_estimate <- function(f,n,sigma,test_set){
  sample <- generate_sample(f,n,sigma)
  model_df2 <- smooth.spline(sample$x, sample$y, df=2)
  predict_df2 <- predict(model_df2, test_set$x)
  
  model_df99 <- smooth.spline(sample$x, sample$y, df=99)
  predict_df99 <- predict(model_df99, test_set$x)
  
  toReturn <- c(predict_df2, predict_df99)
  
  return(toReturn)
}

#Get 10000 estimates for x0
estimatesTest <- mosaic::do(n_sample)*get_estimate(f,n,sigma,test_set)
estimates_df2 <- as.vector(unlist(estimatesTest$y))
estimates_df99 <- as.vector(unlist(estimatesTest$y.1))
estimates <- cbind(estimates_df2,estimates_df99)

#Generate 10000 y-observed (f(x) + epsilon)
y_observed <- mosaic::do(n_sample)*(f(test_set$x) + rnorm(1,mean=0,sd=0.3))
estimates <- estimates %>% cbind(y_observed) %>% rename(y_observed = result)

#Calculating MSE for each model
estimates <- estimates %>% mutate(resid_df2 = (y_observed - estimates_df2)^2, 
                                  resid_df99 = (y_observed-estimates_df99)^2)
MSE <- estimates %>% summarise(MSE_df2 = mean(resid_df2), MSE_df99 =mean(resid_df99))

#Calculating the variance for each model
var_obs <- estimates %>% summarise(var_df2 = var(estimates_df2), 
                                           var_df99 = var(estimates_df99))

#Calculating the squared bias for each model
squared_bias <- estimates %>% summarise(mean_df2 = mean(estimates_df2), 
                                        mean_df99 = mean(estimates_df99)) %>% 
  mutate(bias_df2 = (0.95^2 - mean_df2)^2,bias_df99 = (0.95^2 - mean_df99)^2) %>% 
  select(bias_df2, bias_df99)

#Combining into one data frame
sum <- c(sum_df2 = var_obs$var_df2 + squared_bias$bias_df2 + 0.09,
         sum_df99 = var_obs$var_df99 + squared_bias$bias_df99 + 0.09)

error_tbl <- rbind(df2 = c(MSE$MSE_df2,var_obs$var_df2,squared_bias$bias_df2,0.09, sum[1]), 
                   df99 = c(MSE$MSE_df99,var_obs$var_df99,squared_bias$bias_df99,0.09, sum[2]))
colnames(error_tbl) <- c("MSE","var","bias_sq","irr", "sum")
error_df <- as.data.frame(error_tbl)
```


# Tables

As done in Lec 2.7, for both

* An `lm` regression AKA a `smooth.splines(x, y, df=2)` model fit 
* A `smooth.splines(x, y, df=99)` model fit 

output tables comparing:

|  MSE| bias_squared|   var| irreducible|   sum|
|----:|------------:|-----:|-----------:|-----:|
|     X|           X  |     X |      X |         X |

where `sum = bias_squared + var + irreducible`. You can created cleanly formatted tables like the one above by piping a data frame into `knitr::kable(digits=4)`.

```{r}
error_df %>% kable(digits=4)
```


# Analysis

**Questions**:

1. Based on the topics covered in Lec 2.7, name one possible "sanity check" for your results. Name another if you can.
2. In **two** sentences or less, give a rough sketch of what the procedure would
be to get the breakdown of $$\mbox{MSE}\left[\widehat{f}(x)\right]$$ for *all*
$x$ in this example, and not just for $$\mbox{MSE}\left[\widehat{f}(x_0)\right]
= \mbox{MSE}\left[\widehat{f}(0.95)\right]$$.
3. Which of the two models would you choose for predicting the point of interest and why?

**Answers**:

1. The first sanity check would be to compare my answer to the graphs presented in Lecture 2.7. The graphs give a range that my values should fall inside or near. If I come up with a value far outside the range presented in the graphs, I should reevaluate my calculations and then my method.
2. The process of finding the MSE for all values would look very similar to the process used here. Except, instead of finding the MSE for $x_0=0.95$, we'd find the error using the $mean(y_{observed})$ compared to the $mean(f(x)+ \epsilon)$ over the range of possible x values. 
3. Honestly, I would not use either of these models. $df=2$ is underfit and $df=99$ is overfit. Both have similar error, one due to bias and the other due to variance. A better model would use a df that falls somewhere between 2 and 99.  
Update on 3: Andrew says, "You actually are correct in your assumption. The ideal spline has a # of df between 2 and 99. But if we had to choose between these two, which is slightly better?"  
My response: The splines model with $df=2$ has a slightly lower MSE, so, if you hold a gun to my head, I'll choose this model. However, I must emphasize that choosing either model would be irresponsible and result in a poor prediction relative to the model created with optimal df. 
